{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping von Stellenanzeigen – Korpuserstellung mit Hindernissen\n",
    "\n",
    "**TU Dresden, Institut für Germanistik, Professur für Angewandte Linguistik**\n",
    "\n",
    "***\n",
    "\n",
    "**Seminar:** Programmieren lernen für Geistes- und Sozialwissenschaftler:innen\n",
    "\n",
    "**Dozent:** Yannick Frommherz\n",
    "\n",
    "**Semester der Prüfungsleistung:** Wintersemester 2023/2024\n",
    "\n",
    "**Art der Prüfungsleistung:** Kombinierte Arbeit, Vertiefungsmodul Europäische Sprachen\n",
    "\n",
    "***\n",
    "\n",
    "**Eingereicht von:** Anna Bakker, Master EuroS, Matr.-Nr. 4068462\n",
    "\n",
    "**Datum:** 31.03.2024\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36'}\n",
    "r = requests.get(\"https://www.stepstone.de/jobs/in-Würzburg?radius=5&page=1\", headers=headers, timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import time\n",
    "\n",
    "#‼️ headers als Argument übergeben, um Funktion autark zu machen\n",
    "def search_urls(city, radius, page, headers):\n",
    "    url = \"https://www.stepstone.de/jobs/in-%s?radius=%i&page=%i\" % (city, radius, page, headers) #abzurufende URL mit entsprechenden Platzhaltern\n",
    "    print(url)\n",
    "    #‼️ timeout einbauen\n",
    "    response = requests.get(url, headers=headers, timeout=5) \n",
    "    \n",
    "    ‼️ Handling fehlerhafter Status codes\n",
    "    if not str(response.status_code).startswith(\"2\"):\n",
    "        return(response.status_code)\n",
    "    \n",
    "    quelltext = response.text\n",
    "    \n",
    "    regex = r'href=\"([^\"]*\\/stellenangebote-[^\\s\"]+)\"' \n",
    "    url_matches = re.findall(regex, quelltext)\n",
    "    print(url_matches)\n",
    "    return url_matches\n",
    "\n",
    "cities = [\"Würzburg\"] \n",
    "r = 5\n",
    "til_page = 1 \n",
    "#headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36', \"Upgrade-Insecure-Requests\": \"1\",\"DNT\": \"1\",\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\"Accept-Language\": \"de-DE,de;q=0.5\",\"Accept-Encoding\": \"gzip, deflate\"}\n",
    "#‼️ Abfrage fuktioniert nur mit meinem header..., ggf. zwischen headern rotieren\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36'}\n",
    "matches = [] \n",
    "for c in cities:\n",
    "    print(c)\n",
    "for p in range(1,til_page+1):\n",
    "    print(p)\n",
    "    found_urls = search_urls(c, r, p, headers)\n",
    "    matches.extend(found_urls) \n",
    "\n",
    "for match in matches:\n",
    "     print(match)\n",
    "\n",
    "matches = list(set(matches))\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36'}\n",
    "\n",
    "source_code = requests.get(\"https://www.mdr.de/nachrichten/sachsen-anhalt/landespolitik/hoecke-anklage-interview-kai-langer-gedenkstaetten-100.html\", headers=headers, timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(source_code.text)\n",
    "soup.find(\"span\", class_=\"headline\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition der Funktion für die gewünschten Textblöcke\n",
    "#‼️ nur Extraktion von .text anstatt kompletten Elementen, dann kann man sich auch die ganze Bereinigung sparen\n",
    "def extract(soup_offer_text):\n",
    "    title = \"\"\n",
    "    introduction = \"\"\n",
    "    description = \"\"\n",
    "    profile = \"\"\n",
    "    benefits = \"\"\n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"header-job-title\"}) is None):\n",
    "        return \n",
    "    else:\n",
    "        title = str(soup_offer_text.find(attrs={\"data-at\": \"header-job-title\"}).contents[0]) \n",
    "    \n",
    "    #‼️ ggf. Quelltext erst \"zuschneiden\"\n",
    "\n",
    "#Greift auf den content innerhalb dieses Attributs zu; für jeden Anzeigenabschnitt einzeln\n",
    "#Nimmt den Inhalt, wenn welcher da ist, aus den Attributen und dem span innerhalb der Attribute\n",
    "    #‼️ if soup_offer_text.find(...):\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-introduction-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-introduction-content\"}).span is not None):\n",
    "            introduction = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-introduction-content\"}).span.contents[0]) \n",
    "\n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-description-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-description-content\"}).span is not None):\n",
    "            description = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-description-content\"}).span.contents[0])\n",
    "\n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-profile-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-profile-content\"}).span is not None):\n",
    "            profile = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-profile-content\"}).span.contents[0])\n",
    "\n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-benefits-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-benefits-content\"}).span is not None):\n",
    "            benefits = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-benefits-content\"}).span.contents[0])\n",
    "\n",
    "    #‼️ xml oder csv für Strukturierung geeigneter\n",
    "    texts = [introduction, description, profile, benefits]\n",
    "    text = open(\"rohtext.txt\", \"a\", encoding='utf-16-le')\n",
    "    text.write(\"\\n\" + \"## \")\n",
    "    text.write(\"///\" + title+ \"///\" + \"\\n\")\n",
    "    for x in texts:   \n",
    "        text.write(x + \"\\n\")\n",
    "    text.close()\n",
    "\n",
    "text = open(\"rohtext.txt\", \"w\", encoding='utf-16-le')\n",
    "text.close()\n",
    "i = 0\n",
    "for url in matches: #‼️ tqdm für Fortschrittsanzeige\n",
    "    offer_url = \"https://www.stepstone.de\" + url #die extrahierten Linkbestandteile der Anzeigen werden durch den Stammlink komplettiert\n",
    "\n",
    "    #headers (weil oben schon Yannicks Header) = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36', \"Upgrade-Insecure-Requests\": \"1\",\"DNT\": \"1\",\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\"Accept-Language\": \"de-DE,de;q=0.5\",\"Accept-Encoding\": \"gzip, deflate\"}\n",
    "\n",
    "    sourcecode_offer = requests.get(offer_url, headers=headers).text\n",
    "    soup_offer = BeautifulSoup(sourcecode_offer, \"lxml\")\n",
    "\n",
    "    extract(soup_offer)\n",
    "    i += 1\n",
    "    \n",
    "    print(str(i) + \"/\" + str(len(matches)) + \"---\" + format((i/len(matches))* 100, '.2f') + \"%\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stellenanzeigen = []\n",
    "with open(\"Dachau.txt\", encoding='utf-16-le') as f:\n",
    "    dachau = f.read()\n",
    "    for line in dachau.split(\"##\"):\n",
    "        stellenanzeigen.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([], columns=[\"Titel\", \"Text\", \"Form\", \"Ort\", \"Datum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for s in stellenanzeigen:\n",
    "    if len(s) == 0:\n",
    "        continue\n",
    "    elemente = s.split(\"///\")\n",
    "    df.loc[i] = [elemente[1], elemente[2], \"\", \"Dachau\", \"Ende März\"]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(single_text):\n",
    "    #hier müsste die Lemmatisierung hin \n",
    "    return single_text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eintragen von Werten basierend auf Bedingungen\n",
    "x = df.Titel\n",
    "if_list = [x.str.contains('.\\*(\\s|\\))'), x.str.contains('\\w*(\\*|:|_)\\s*in')] \n",
    "then_list = [\"Sternchen\", \"in-Form\"]\n",
    "df[\"Form\"] = np.select(if_list, then_list, \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Lemmata\"] = df.Text.apply(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Form.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gruppieren und Verteilung berechnen\n",
    "df.groupby(\"Ort\").Form.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtern\n",
    "df[df.Form == \"in-Form\"].Titel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "#‼️ Funktion!\n",
    "def extract(soup_offer_text):\n",
    "    title = \"\"\n",
    "    introduction = \"\"\n",
    "    description = \"\"\n",
    "    profile = \"\"\n",
    "    benefits = \"\"\n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"header-job-title\"}) is None):\n",
    "        return \n",
    "    else:\n",
    "        title = str(soup_offer_text.find(attrs={\"data-at\": \"header-job-title\"}).contents[0]) \n",
    "    \n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-introduction-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-introduction-content\"}).span is not None):\n",
    "            introduction = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-introduction-content\"}).span.contents[0]) \n",
    "\n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-description-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-description-content\"}).span is not None):\n",
    "            description = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-description-content\"}).span.contents[0])\n",
    "\n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-profile-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-profile-content\"}).span is not None):\n",
    "            profile = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-profile-content\"}).span.contents[0])\n",
    "\n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-benefits-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-benefits-content\"}).span is not None):\n",
    "            benefits = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-benefits-content\"}).span.contents[0])\n",
    "\n",
    "    texts = [introduction, description, profile, benefits]\n",
    "    text = open(\"rohtext.txt\", \"a\", encoding='utf-16-le')\n",
    "    text.write(\"\\n\" + \"## \")\n",
    "    text.write(\"///\" + title+ \"///\" + \"\\n\")\n",
    "    for x in texts:   \n",
    "        text.write(x + \"\\n\")\n",
    "    text.close()\n",
    "\n",
    "text = open(\"rohtext.txt\", \"w\", encoding='utf-16-le')\n",
    "text.close()\n",
    "i = 0\n",
    "\n",
    "with open(\"Links_manuell.txt\") as read_file:\n",
    "     file = read_file.read()  \n",
    "for url in file.split('\\n'): \n",
    "    offer_url = url \n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36', \"Upgrade-Insecure-Requests\": \"1\",\"DNT\": \"1\",\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\"Accept-Language\": \"de-DE,de;q=0.5\",\"Accept-Encoding\": \"gzip, deflate\"}\n",
    "    \n",
    "    matches = 500 \n",
    "\n",
    "    sourcecode_offer = requests.get(url, headers=headers).text\n",
    "    soup_offer = BeautifulSoup(sourcecode_offer, \"lxml\")\n",
    "\n",
    "    extract(soup_offer)\n",
    "    i += 1\n",
    "    print(str(i) + \"/\" + str(matches) + \"---\" + format((i/matches)* 100, '.2f') + \"%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#‼️ Schritte tlws. redudant, wenn mit .text-Attribut gearbeitet worden wäre, da inline-Tags dadurch automatisch übersprungen werden\n",
    "#‼️ Arbeit mit regex aber lobenswert\n",
    "\n",
    "import re \n",
    "\n",
    "#‼️ with-Statement benutzen\n",
    "text = open(\"rohtext.txt\", \"r\", encoding='utf-16-le') \n",
    "lines = text.readlines()\n",
    "\n",
    "korpus = \"\".join(lines)\n",
    "    \n",
    "remove = [\"<p>\", \"</p>\", \"<strong>\", \"</strong>\", \"<ul>\", \"</ul>\", \"<li>\", \"</li>\", \"<em>\", \"</em>\", \"<br/>\", \"</a>\",\"<b>\", \"</b>\", \"<u>\", \"</u>\", \"<i>\", \"</i>\"]\n",
    "for x in remove:\n",
    "    korpus = korpus.replace(x, \" \")\n",
    "\n",
    "korpus = re.sub(r'<a href=\".*\">', \"\", korpus)\n",
    "\n",
    "korpus = re.sub(r'<h1>.*</h1>', \"\", korpus)\n",
    "\n",
    "korpus = korpus.replace(\"\\n\", \" \")\n",
    "\n",
    "korpus = korpus.replace(\"   \", \" \")\n",
    "while \"  \" in korpus:\n",
    "    korpus = korpus.replace(\"  \", \" \")\n",
    "\n",
    "korpus = korpus.replace(\"„\", \"\\\"\")\n",
    "korpus = korpus.replace(\"“\", \"\\\"\")\n",
    "korpus = korpus.replace(\"·\", \"-\")\n",
    "korpus = korpus.replace(\"‑\",\"-\")\n",
    "korpus = korpus.replace(\"’\",\"'\")\n",
    "korpus = korpus.replace(\"…\",\"...\")\n",
    "korpus = korpus.replace(\"\\t\",\" \")\n",
    "\n",
    "korpus = re.sub(r'[^a-zA-Z0-9äöü.,;ß/()\\[\\]\\-#\\|&\\*\\+:ÖÄÜ?\"!§$%&\\\\²³€\\'\\s]', \"\", korpus)\n",
    "\n",
    "text = open(\"Dachau.txt\", \"w\", encoding='utf-16-le')   \n",
    "text.write(korpus)\n",
    "text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#‼️ für Datenanalyse effizienter: pandas\n",
    "import re\n",
    "\n",
    "korpus = open(\"Dachau.txt\", \"r\", encoding='utf-16-le') \n",
    "lines = korpus.readlines()\n",
    "\n",
    "korpus = \"\".join(lines)\n",
    "korpus = korpus.strip()\n",
    "\n",
    "korpus = korpus.split(\"##\")\n",
    "print(len(korpus))\n",
    "job_text = {} \n",
    "korpus.remove('') \n",
    "\n",
    "title_regex = r'///.*///'\n",
    "for x in korpus:\n",
    "    inseration = x.strip()\n",
    "    title = str(re.findall(title_regex, inseration).pop())\n",
    "\n",
    "    inseration = inseration.replace(title,\"\")\n",
    "    inseration.strip()\n",
    "    title = title.replace(\"///\",\"\")\n",
    "    job_text[title] = inseration\n",
    "num_jobs = len(job_text)\n",
    "\n",
    "\n",
    "print(\"sternchen\") \n",
    "sternchen_dict = {}\n",
    "sternchen_regex =r'.\\*(\\s|\\))'\n",
    "\n",
    "for title in job_text:\n",
    "    if len(re.findall(sternchen_regex, title)) != 0:\n",
    "        print(title)\n",
    "        sternchen_dict[title] = job_text[title]\n",
    "\n",
    "for title in sternchen_dict:\n",
    "    job_text.pop(title, None)\n",
    "print(\"-----\") \n",
    "\n",
    "print(\"in-Formen\")\n",
    "in_dict = {} \n",
    "in_regex = r'\\w*(\\*|:|_)\\s*in'\n",
    "\n",
    "for title in job_text:\n",
    "    if len(re.findall(in_regex, title)) != 0:\n",
    "        print(title)\n",
    "        in_dict[title] = job_text[title]\n",
    "\n",
    "for title in in_dict:\n",
    "    job_text.pop(title, None)\n",
    "print(\"-----\")     \n",
    "\n",
    "print(\"m/d/w-Formen\")\n",
    "mwd_dict = {} \n",
    "mwd_regex = r'(\\(|\\s)(\\w(\\/|\\||,)\\s*\\w(\\/|\\||,)\\s*\\w)|(\\w(\\/|\\||,)\\s*\\w)(\\)|\\s)'\n",
    "\n",
    "for title in job_text:\n",
    "    \n",
    "    if len(re.findall(mwd_regex, title)) != 0:\n",
    "        print(title)\n",
    "        mwd_dict[title] = job_text[title]\n",
    "\n",
    "\n",
    "for title in mwd_dict:\n",
    "    job_text.pop(title, None)\n",
    "print(\"-----\")\n",
    "\n",
    "print(\"Rest\")\n",
    "for title in job_text:\n",
    "    print(title)\n",
    "print(\"Jobs insg.: \" + str(num_jobs))\n",
    "print(\"Jobs Sternchen: \" + str(len(sternchen_dict)))\n",
    "print(\"Jobs *_:in: \" + str(len(in_dict)))\n",
    "print(\"Jobs m/w/d: \" + str(len(mwd_dict)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
