{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping von Stellenanzeigen – Korpuserstellung mit Hindernissen\n",
    "\n",
    "**TU Dresden, Institut für Germanistik, Professur für Angewandte Linguistik**\n",
    "\n",
    "***\n",
    "\n",
    "**Seminar:** Programmieren lernen für Geistes- und Sozialwissenschaftler:innen\n",
    "\n",
    "**Dozent:** Yannick Frommherz\n",
    "\n",
    "**Semester der Prüfungsleistung:** Wintersemester 2023/2024\n",
    "\n",
    "**Art der Prüfungsleistung:** Kombinierte Arbeit, Vertiefungsmodul Europäische Sprachen\n",
    "\n",
    "***\n",
    "\n",
    "**Eingereicht von:** Anna Bakker, Master EuroS, Matr.-Nr. 4068462\n",
    "\n",
    "**Datum:** 31.03.2024\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worum geht's?\n",
    "***\n",
    "\n",
    "Die vorliegende Projektarbeit soll den Aufbau eines geeigneten Skripts zeigen, mit dem Websites gescraped und somit beliebig große Textmengen gesammelt werden können, um zunächst ein nach eigenen Parametern aufgebautes Textkorpus zu erstellen. Als konkreter Anwendungsfall werden die Titel und Texte diverser Stellenanzeigen des Jobportals StepStone extrahiert und in einer Datei zusammengestellt. Die Textmenge soll sodann von irrelevanten Zeichen bereinigt und so aufbereitet werden, dass ein zur Untersuchung entsprechender Fragestellungen geeignetes Korpus entsteht. Dieses kann dann mithilfe der Mittel der Datenanalyse auf verschiedene Aspekte hin untersucht und modifiziert werden. In diesem Beispiel sollen die verschiedenen Arten, Stellenanzeigen zu Gendern, untersucht werden. Da dies erst einmal primär die Titel betrifft, müssen Titel und Textkörper getrennt voneinander analysierbar sein. Zudem muss beachtet werden, dass bei der Bereinigung von Sonderzeichen nicht die gendermarkierenden Zeichen mit entfernt werden. Im Anschluss werden dann noch einige Ideen für die Analyse vorgestellt und das Projekt dann mit einer kurzen Reflexion abgeschlossen. Das Skript ist in verschiedene Codeblöcke unterteilt, sodass die einzelnen Schritte nicht notwendigerweise alle nacheinander stattfinden müssen und die Codeblöcke zur Bereinigung und Aufbereitung von Texten auch unabhängig vom Scraping-Teil ausgeführt werden können. Dies hat noch den weiteren Vorteil, dass dadurch auch die Anzahl der Anfragen an die Website minimiert werden kann, was relevant ist, um Blockierungen durch die Website vorzubeugen. \n",
    "\n",
    "\n",
    "## Erste Hälfte – Der lange Weg zum Rohtext\n",
    "***\n",
    "\n",
    "Der erste Teil besteht darin, die Website über das Modul *requests* aufzurufen, den html-Quellcode mit *Beautiful Soup* zu parsen, die gewünschten Textbestandteile zu extrahieren und schließlich in eine Datei zu überführen, welche dann weiterverarbeitet werden kann. Da dieser Schritt erst einmal auf den gesamten Quellcode der Seite zugreift, geht es danach darum, sich mit dem Quellcode der Website und dessen html-Struktur genauer vertraut zu machen. Dabei muss dem Tagging besondere Aufmerksamkeit gewidmet werden, um herauszufinden, welche Teile des umfangreichen Quelltextes die spezifischen Titel und Textpassagen der Stellenanzeigen enthalten und welche Links jeweils darauf verweisen. Sobald die Linkstruktur identifiziert ist, kann ein geeigneter Regulärer Ausdruck (*RegEx*) gefunden werden, mit welchem sämtliche relevante Links aus dem Quelltext extrahiert und im Anschluss wieder \"zusammengebaut\" werden. Nun wird über diese Links iteriert, um auf die einzelnen Anzeigen zuzugreifen und – nach Identifikation der entsprechenden Tags – die jeweiligen Abschnitte der konkreten Stellenbeschreibungen zu entnehmen und in ein Dokument zu überführen. Da nicht willkürlich alle Anzeigen der gesamten Website abgerufen werden sollen, sondern nur diejenigen, welchen den gewünschten Suchkriterien entsprechen, werden Parameter eingeführt, welche so auch als Suchfilter auf der Website selbst zur Verfügung stehen: Stadt und Radius sowie die Angabe, bis zu welcher Seite die Anzeigen abgerufen werden sollen. Natürlich könnten auch weitere Parameter, wie die Berufsbezeichnung, implementiert werden. Da das Ziel der vorliegenden Arbeit jedoch ein Überblick über sämtliche Berufsgruppen schaffen soll, ist dies erst einmal irrelevant. Sämtliche Such- und Filtermöglichkeiten, welche die Website bietet, könnten auch im Code abgebildet werden und somit die Abfrage modifizieren.\n",
    "\n",
    "Für dieses Vorgehen werden diverse Module benötigt, die zunächst importiert werden müssen: *re* für die RegEx, welche die Grundstruktur der gesuchten Links abbilden können und so ein Suchmuster für eine effiziente Ausgabe bieten. Zudem *BeautifulSoup*, eine Programmbibliothek, mit welcher sich html-Dokumente parsen lassen. *requests* ermöglicht erst das Senden von HTTP-Anfragen und eine Aufbereitung der entsprechenden Antworten der Website. Das letzte Modul, *time*, gehört in den Bereich des Troubleshootings und wird im weiteren Verlauf erläutert.\n",
    "\n",
    "Beim Webscraping kann es zu verschiedenen Problemen kommen, die es im Blick zu behalten gilt. Viele Seiten haben Maßnahmen implementiert, um automatisiertes Zugreifen zu blockieren. Dies umfasst unter anderem rate limiting, Bot-Erkennung oder das Blocken von IP-Adressen (vgl. Emre: 2023). Ein nicht unwesentlicher Teil dieses ersten Blocks wird sich also auch mit Vorbeugung dieser Effekte sowie mit Troubleshooting und Lösungsansätzen befassen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konstruktion der Links und Modifikation der Abfrage\n",
    "\n",
    "Zuerst müssen die bereits beschriebenen Module importiert werden. Als nächstes wird eine Funktion definiert, welche die für diese Arbeit relevanten Parameter Stadt, Radius und Seitenzahl enthält. In diesem Schritt wäre es möglich, die unterschiedlichsten Parameter einzufügen, wenn die dafür zuständige Kennzeichnung im Link der Website erkennbar ist. Diese Stelle wird dann durch einen Platzhalter in der Funktion ersetzt, mit welchem auch festgelegt wird, welcher Zeichentyp eingetragen werden muss: in diesem Falle ein String für die Stadt und Ganzzahlen für Radius und Seitenzahl. \n",
    "Über *requests* wird dann die URL aufgerufen und auf den Quelltext zugegriffen. Dieser Schritt birgt das meiste Problempotential, da an dieser Stelle die Blockmechanismen und Fallen der Website greifen. Um das zu umgehen, wurde ein Fake-Header in die Abfrage eingebaut (Ikleiw: 2020), welcher im folgenden Abschnitt definiert wird und dafür sorgt, dass Herkunft und Art der Anfrage verschleiert und eine menschliche Nutzung nachgeahmt wird. Über einen passenden RegEx werden dann alle Links zu den gewünschten Anzeigen gefunden, ausgegeben und der Liste \"matches\" hinzugefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Würzburg\n",
      "1\n",
      "https://www.stepstone.de/jobs/in-Würzburg?radius=5&page=1\n",
      "['/stellenangebote--Account-Managerin-Deutschland-Region-Sued-m-w-d-Wuerzburg-Truck-line-GmbH--10928444-inline.html', '/stellenangebote--Aussendienstmitarbeiter-als-selbststaendiger-Handelsvertreter-m-w-d-Nuernberg-Wuerzburg-Stroeer-Media-Deutschland-GmbH--10887084-inline.html', '/stellenangebote--Vertriebsmitarbeiter-Aussendienst-in-der-Neukundenakquise-m-w-d-Wuerzburg-Stroeer-Media-Deutschland-GmbH--10887087-inline.html', '/stellenangebote--Elektroniker-Schutz-und-Stationsleittechnik-Strom-m-w-d-Wuerzburg-Wuerzburger-Versorgungs-und-Verkehrs-GmbH--10887353-inline.html', '/stellenangebote--Vertriebsmitarbeiter-Aussendienst-in-der-Neukundenakquise-m-w-d-Wuerzburg-Stroeer-Media-Deutschland-GmbH--10887086-inline.html', '/stellenangebote--Chefarzt-m-w-d-Gynaekologie-Geburtshilfe-fuer-die-Klinik-fuer-Gynaekologie-Geburtshilfe-Wuerzburg-Klinikum-Wuerzburg-Mitte-gGmbH--10928083-inline.html', '/stellenangebote--Vermoegenskundenberater-m-w-d-Wuerzburg-Plauen-Bad-Neustadt-und-Meiningen-MERKUR-PRIVATBANK--9959590-inline.html', '/stellenangebote--Sachbearbeiter-m-w-d-Bereitschafts-und-Notarztdienst-in-Teilzeit-30-Std-Wo-Wuerzburg-Kassenaerztliche-Vereinigung-Bayerns--10928292-inline.html', '/stellenangebote--Sales-Development-Representative-m-w-d-Stuttgart-Nuernberg-Muenchen-Wuerzburg-Augsburg-Ulm-HF-Solutions-GmbH--10923964-inline.html', '/stellenangebote--Kaufm-Mitarbeiter-im-Vertriebsinnendienst-m-w-d-Veitshoechheim-Wuerzburg-WEGMANN-automotive-GmbH--10926551-inline.html', '/stellenangebote--Berater-fuer-den-Gesundheitsmarkt-m-w-d-in-Voll-oder-Teilzeit-Wuerzburg-Deutsche-Apotheker-und-Aerztebank-eG-apoBank--10925674-inline.html', '/stellenangebote--Assistenz-der-Geschaeftsfuehrung-m-w-d-Wuerzburg-Veitshoechheim-WEGMANN-automotive-GmbH--10926549-inline.html', '/stellenangebote--SAP-Consultant-Demand-Manager-m-w-d-Veitshoechheim-Wuerzburg-WEGMANN-automotive-GmbH--10926542-inline.html', '/stellenangebote--Vertriebsmitarbeiter-im-Aussendienst-Supervisor-m-w-d-Sportnahrung-Frankfurt-Wuerzburg-Leipzig-Dresden-Halle-an-der-Saale-NaskorSports--10928024-inline.html', '/stellenangebote--Vertriebsmitarbeiter-im-Aussendienst-m-w-d-Sportnahrung-Kaiserslautern-Mainz-Mannheim-Frankfurt-Wuerzburg-NaskorSports--10928026-inline.html', '/stellenangebote--Elektroniker-Parkierungsanlagen-m-w-d-Wuerzburg-Wuerzburger-Versorgungs-und-Verkehrs-GmbH--10927962-inline.html', '/stellenangebote--Projektmanager-Strom-m-w-d-Wuerzburg-Wuerzburger-Versorgungs-und-Verkehrs-GmbH--10927987-inline.html', '/stellenangebote--Projektmanager-Gas-Wasser-Fernwaerme-m-w-d-Wuerzburg-Wuerzburger-Versorgungs-und-Verkehrs-GmbH--10927975-inline.html', '/stellenangebote--Kaufmaennischer-Mitarbeiter-fuer-Vertrieb-und-Service-w-m-d-Bayreuth-Erlangen-Wuerzburg-Nuernberg-Schweinfurt-ADAC-Nordbayern-e-V--9175182-inline.html', '/stellenangebote--Senior-Projektmanager-Hochbau-all-genders-Wuerzburg-WSP-Deutschland-AG--10878423-inline.html', '/stellenangebote--Global-Sourcing-Manager-Nachunternehmer-m-w-d-Hybrid-Wuerzburg-Iqony-Solar-Energy-Solutions-GmbH--10656815-inline.html', '/stellenangebote--Ingenieur-Gas-und-Wasserbetrieb-m-w-d-Wuerzburg-Wuerzburger-Versorgungs-und-Verkehrs-GmbH--10924315-inline.html', '/stellenangebote--Meister-Netzleitstelle-m-w-d-Wuerzburg-Wuerzburger-Versorgungs-und-Verkehrs-GmbH--10924403-inline.html', '/stellenangebote--Ingenieur-Systemtechnik-m-w-d-Wuerzburg-Wuerzburger-Versorgungs-und-Verkehrs-GmbH--10924320-inline.html', '/stellenangebote--Softwareentwickler-SPS-m-w-d-Wertheim-Grossraum-Wuerzburg-Ersa-GmbH--10922545-inline.html']\n",
      "/stellenangebote--Account-Managerin-Deutschland-Region-Sued-m-w-d-Wuerzburg-Truck-line-GmbH--10928444-inline.html\n",
      "/stellenangebote--Aussendienstmitarbeiter-als-selbststaendiger-Handelsvertreter-m-w-d-Nuernberg-Wuerzburg-Stroeer-Media-Deutschland-GmbH--10887084-inline.html\n",
      "/stellenangebote--Vertriebsmitarbeiter-Aussendienst-in-der-Neukundenakquise-m-w-d-Wuerzburg-Stroeer-Media-Deutschland-GmbH--10887087-inline.html\n",
      "/stellenangebote--Elektroniker-Schutz-und-Stationsleittechnik-Strom-m-w-d-Wuerzburg-Wuerzburger-Versorgungs-und-Verkehrs-GmbH--10887353-inline.html\n",
      "/stellenangebote--Vertriebsmitarbeiter-Aussendienst-in-der-Neukundenakquise-m-w-d-Wuerzburg-Stroeer-Media-Deutschland-GmbH--10887086-inline.html\n",
      "/stellenangebote--Chefarzt-m-w-d-Gynaekologie-Geburtshilfe-fuer-die-Klinik-fuer-Gynaekologie-Geburtshilfe-Wuerzburg-Klinikum-Wuerzburg-Mitte-gGmbH--10928083-inline.html\n",
      "/stellenangebote--Vermoegenskundenberater-m-w-d-Wuerzburg-Plauen-Bad-Neustadt-und-Meiningen-MERKUR-PRIVATBANK--9959590-inline.html\n",
      "/stellenangebote--Sachbearbeiter-m-w-d-Bereitschafts-und-Notarztdienst-in-Teilzeit-30-Std-Wo-Wuerzburg-Kassenaerztliche-Vereinigung-Bayerns--10928292-inline.html\n",
      "/stellenangebote--Sales-Development-Representative-m-w-d-Stuttgart-Nuernberg-Muenchen-Wuerzburg-Augsburg-Ulm-HF-Solutions-GmbH--10923964-inline.html\n",
      "/stellenangebote--Kaufm-Mitarbeiter-im-Vertriebsinnendienst-m-w-d-Veitshoechheim-Wuerzburg-WEGMANN-automotive-GmbH--10926551-inline.html\n",
      "/stellenangebote--Berater-fuer-den-Gesundheitsmarkt-m-w-d-in-Voll-oder-Teilzeit-Wuerzburg-Deutsche-Apotheker-und-Aerztebank-eG-apoBank--10925674-inline.html\n",
      "/stellenangebote--Assistenz-der-Geschaeftsfuehrung-m-w-d-Wuerzburg-Veitshoechheim-WEGMANN-automotive-GmbH--10926549-inline.html\n",
      "/stellenangebote--SAP-Consultant-Demand-Manager-m-w-d-Veitshoechheim-Wuerzburg-WEGMANN-automotive-GmbH--10926542-inline.html\n",
      "/stellenangebote--Vertriebsmitarbeiter-im-Aussendienst-Supervisor-m-w-d-Sportnahrung-Frankfurt-Wuerzburg-Leipzig-Dresden-Halle-an-der-Saale-NaskorSports--10928024-inline.html\n",
      "/stellenangebote--Vertriebsmitarbeiter-im-Aussendienst-m-w-d-Sportnahrung-Kaiserslautern-Mainz-Mannheim-Frankfurt-Wuerzburg-NaskorSports--10928026-inline.html\n",
      "/stellenangebote--Elektroniker-Parkierungsanlagen-m-w-d-Wuerzburg-Wuerzburger-Versorgungs-und-Verkehrs-GmbH--10927962-inline.html\n",
      "/stellenangebote--Projektmanager-Strom-m-w-d-Wuerzburg-Wuerzburger-Versorgungs-und-Verkehrs-GmbH--10927987-inline.html\n",
      "/stellenangebote--Projektmanager-Gas-Wasser-Fernwaerme-m-w-d-Wuerzburg-Wuerzburger-Versorgungs-und-Verkehrs-GmbH--10927975-inline.html\n",
      "/stellenangebote--Kaufmaennischer-Mitarbeiter-fuer-Vertrieb-und-Service-w-m-d-Bayreuth-Erlangen-Wuerzburg-Nuernberg-Schweinfurt-ADAC-Nordbayern-e-V--9175182-inline.html\n",
      "/stellenangebote--Senior-Projektmanager-Hochbau-all-genders-Wuerzburg-WSP-Deutschland-AG--10878423-inline.html\n",
      "/stellenangebote--Global-Sourcing-Manager-Nachunternehmer-m-w-d-Hybrid-Wuerzburg-Iqony-Solar-Energy-Solutions-GmbH--10656815-inline.html\n",
      "/stellenangebote--Ingenieur-Gas-und-Wasserbetrieb-m-w-d-Wuerzburg-Wuerzburger-Versorgungs-und-Verkehrs-GmbH--10924315-inline.html\n",
      "/stellenangebote--Meister-Netzleitstelle-m-w-d-Wuerzburg-Wuerzburger-Versorgungs-und-Verkehrs-GmbH--10924403-inline.html\n",
      "/stellenangebote--Ingenieur-Systemtechnik-m-w-d-Wuerzburg-Wuerzburger-Versorgungs-und-Verkehrs-GmbH--10924320-inline.html\n",
      "/stellenangebote--Softwareentwickler-SPS-m-w-d-Wertheim-Grossraum-Wuerzburg-Ersa-GmbH--10922545-inline.html\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import time\n",
    "\n",
    "def search_urls(city, radius, page):\n",
    "    url = \"https://www.stepstone.de/jobs/in-%s?radius=%i&page=%i\" % (city, radius, page) #abzurufende URL mit entsprechenden Platzhaltern\n",
    "    print(url)\n",
    "    quelltext = requests.get(url, headers=headers).text #http-Anfrage unter falschem Header\n",
    "\n",
    "    #RegEx, welcher die gewünschten Links matcht; diese werden dann gesucht und ausgegeben\n",
    "    regex = r'href=\"([^\"]*\\/stellenangebote-[^\\s\"]+)\"' \n",
    "    url_matches = re.findall(regex, quelltext)\n",
    "    print(url_matches)\n",
    "    return url_matches\n",
    "\n",
    "#die Variablen werden initiiert und die gewünschten Parameter eingetragen\n",
    "cities = [\"Würzburg\"] \n",
    "r = 5\n",
    "til_page = 1 \n",
    "#menschliche Nutzung nachahmender Fake-Header\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36', \"Upgrade-Insecure-Requests\": \"1\",\"DNT\": \"1\",\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\"Accept-Language\": \"de-DE,de;q=0.5\",\"Accept-Encoding\": \"gzip, deflate\"}\n",
    "matches = [] \n",
    "for c in cities:\n",
    "   print(c)\n",
    "for p in range(1,til_page+1):\n",
    "        print(p)\n",
    "        found_urls = search_urls(c, r, p)\n",
    "        matches.extend(found_urls) #die gefundenen URLs werden der matches-Liste hinzugefügt\n",
    "\n",
    "\n",
    "for match in matches:\n",
    "     print(match)\n",
    "\n",
    "#entfernt eventuelle Dopplungen in den Links\n",
    "matches = list(set(matches))\n",
    "print(len(matches))\n",
    "#Stamm-URL, Stadt, Seitenzahl und entsprechende Matches werden ausgegeben\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Der Problemlösung erster Teil: Finde den Fehler\n",
    "\n",
    "Es werden also der Linkstamm sowie die jeweiligen Erweiterungen der Links für alle den Parametern entsprechenden Stellenanzeigen ausgegeben sowie die Anzahl der gefundenen matches, um überprüfen zu können, ob alles gefunden wurde.  \n",
    "Was in der Theorie gut klingt, führte in der Umsetzung zu diversen Problemen, welche nicht immer identifiziert werden konnten und manchmal von einer Ausführung des Codes zur nächsten scheinbar willkürlich wechselten. So kam es beim ersten Versuch zu gar keinem Ergebnis, die Anfrage lief unbegrenzt weiter, bis zum Abbruch. Es wurde keine Fehlermeldung ausgegeben. Auch eine versuchte Ausgabe des Status Codes oder Inhalte der response zur Identifikation des Problems verliefen erfolglos. Ein Testdurchlauf mit einer anderen Website ergab, dass es nicht am Code lag, da dort die Ausführung das gewünschte Ergebnis lieferte. Ein weiterer Versuch einige Tage später ergab eine lange Fehlermeldung mit dem Hinweis, die Verbindung sei unerwartet unterbrochen worden. Beim dritten Versuch schließlich enthielt die Ausgabe nur die Stammlinks der Übersichtsseiten, nicht jedoch die der Stellenanzeigen. Es kann angenommen werden, dass diese Ergebnisse durch Blockingmechanismen der Website zustande kamen. Deswegen wurde zunächst das *time*-Modul implementiert, welches Pausen von beliebig vielen Sekunden zwischen die Abfragen einbaut, um zu kaschieren, dass es sich um ein automatisiertes Abrufen handelt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codeschnipsel, die zum Troubleshooting verwendet wurden:\n",
    "\n",
    "response = requests.get(url) #Überprüfung, ob überhaupt auf die URL zugegriffen werden kann\n",
    "\n",
    "#Falls nicht, können der Status Code, die Response Header und der Response Content ausgegeben werden, um das Problem zu identifizieren:\n",
    "print('Response Status Code:', response.status_code)\n",
    "\n",
    "print('Response Headers:')\n",
    "for header, value in response.headers.items():\n",
    "    print(f'{header}: {value}')\n",
    "\n",
    "print('Response Content:', response.text)\n",
    "\n",
    "time.sleep(10) #baut eine Pause von (x) Sekunden zwischen den Anfragen ein\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem das Problem weiterhin nicht identifiziert werden konnte, sollte es vom anderen Ende her betrachtet werden: der IP, von der aus die Anfrage gesendet wird und welche entsprechend von der Website geblockt wird. Nachdem der Zugriff über einen klassischen VPN-Client ebenfalls keine Erfolge erzielte, funktionierte die Abfrage über das Uninetzwerk *eduroam* zwischenzeitlich interessanterweise auch ohne weitere Schritte zum Umgehen der Blockierung. Somit bot sich die Nutzung des OpenVPN-Zugangs zum Uninetzwerk an, welches es ermöglichte, auch über andere Internetverbindungen erfolgreiche Abfragen generieren zu können. Warum das temporär funktionierte, konnte unbefriedigenderweise leider bis zum Zeitpunkt der Abgabe dieser Arbeit nicht ergründet werden.\n",
    "\n",
    "### Der Problemlösung zweiter Teil: Der Kompromiss\n",
    "\n",
    "Nach einigen Tagen wurden dann jedoch auch die Anfragen über das Uni-VPN blockiert und die Verbindung während der Abfragen abgebrochen, sodass keine Ausgabe möglich war. Eine Suche bei *stack overflow* ergab, dass ein Fake-Header in die Anfrage eingebaut werden kann, um zu verschleiern, dass es sich um ein Skript handelt. Dieser wird initialisiert und dann für die HTTP-Anfrage mit abgerufen. In Kombination mit der Nutzung des Uni-VPN führte diese Strategie zu einem Teilerfolg. Die Anfragen wurden nun zwar nicht mehr geblockt und es wurden die gewünschten Links und der Quellcode ausgegeben, der neue Header führte jedoch zu neuen Problemen: die Ausgabe entsprach nicht mehr den Anzeigen, welche auf der Website selbst bei gleicher Anfrage angezeigt wurden, die Reihenfolge wurde vertauscht, Teile fehlten. Durch Probieren konnte herausgefunden werden, dass die jeweils erste Seite der Anzeigen (also til_page=1 im ersten Block) korrekt ausgegeben wird. Beim Abruf mehrerer Seiten kommt es dann jedoch zu den genannten Fehlern. Häufig wurde auch einfach mehrfach der Quellcode der ersten Seite ausgegeben oder es führte zum Abbruch der Anfrage. Das Problem scheint also im korrekten Durchblättern der Seiten zu liegen. Leider konnte dafür keine Lösung gefunden werden. \n",
    "\n",
    "So muss der Weg also um das Problem herum führen, falls für das gewünschte Korpus nicht bloß 25 Anzeigen pro Stadt ausreichen, denn das ist die Anzahl pro Seite. Eine größere Menge Anzeigen könnte also entweder abgerufen werden, indem die Zahl der Städte erhöht wird und dann jeweils die erste Seite pro Stadt abgerufen wird. Diese Möglichkeit kann mit dem ersten Block des Skripts automatisiert und fehlerfrei in kurzer Zeit ausgeführt werden. Sollen jedoch mehr als 25 Anzeigen einer Stadt abgerufen werden, muss zu einer etwas uneleganten und auch zeitintensiveren Methode gegriffen werden. Das automatisierte Identifizieren und Sammeln der Anzeigenlinks im ersten Block fiele in diesem Falle komplett weg und kann gelöscht oder auskommentiert werden, da zumindest die hier untersuchte Seite StepStone offenbar keine größeren Abfragen erlaubt. Es muss beachtet werden, dass die für den weiteren Verlauf benötigten Module dann ebenfalls nicht importiert werden und dies dann in den folgenden Codeblöcken geschehen muss. \n",
    "\n",
    "Um wenigstens die Extraktion und Bereinigung des Quelltexts sowie die Analyse automatisiert durchführen zu können, können die Links, über welche iteriert werden soll, manuell gesammelt und eingelesen werden, um das Problem des ersten Blocks zu umgehen. Dazu müssen sie nach der entsprechenden Suchanfrage direkt von der Website kopiert und in ein Textdokument eingefügt werden. Wie der Code dafür angepasst werden muss, wird im übernächsten Block gezeigt. Der Rest des Skripts kann dann wie beabsichtigt genutzt werden. Zunächst jedoch die Version für den ursprünglich intendierten Vorgang zur Iteration der automatisch gesammelten Links und der Extraktion der relevanten Quelltextbestandteile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/25---4.00%\n",
      "2/25---8.00%\n",
      "3/25---12.00%\n",
      "4/25---16.00%\n",
      "5/25---20.00%\n",
      "6/25---24.00%\n",
      "7/25---28.00%\n",
      "8/25---32.00%\n",
      "9/25---36.00%\n",
      "10/25---40.00%\n",
      "11/25---44.00%\n",
      "12/25---48.00%\n",
      "13/25---52.00%\n",
      "14/25---56.00%\n",
      "15/25---60.00%\n",
      "16/25---64.00%\n",
      "17/25---68.00%\n",
      "18/25---72.00%\n",
      "19/25---76.00%\n",
      "20/25---80.00%\n",
      "21/25---84.00%\n",
      "22/25---88.00%\n",
      "23/25---92.00%\n",
      "24/25---96.00%\n",
      "25/25---100.00%\n"
     ]
    }
   ],
   "source": [
    "#Definition der Funktion für die gewünschten Textblöcke\n",
    "def extract(soup_offer_text):\n",
    "    title = \"\"\n",
    "    introduction = \"\"\n",
    "    description = \"\"\n",
    "    profile = \"\"\n",
    "    benefits = \"\"\n",
    "#nur dann Extraktion der Inhalte, wenn Titel vorhanden\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"header-job-title\"}) is None):\n",
    "        return \n",
    "    else:\n",
    "        title = str(soup_offer_text.find(attrs={\"data-at\": \"header-job-title\"}).contents[0]) \n",
    "    \n",
    "\n",
    "#Greift auf den content innerhalb dieses Attributs zu; für jeden Anzeigenabschnitt einzeln\n",
    "#Nimmt den Inhalt, wenn welcher da ist, aus den Attributen und dem span innerhalb der Attribute\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-introduction-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-introduction-content\"}).span is not None):\n",
    "            introduction = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-introduction-content\"}).span.contents[0]) \n",
    "\n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-description-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-description-content\"}).span is not None):\n",
    "            description = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-description-content\"}).span.contents[0])\n",
    "\n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-profile-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-profile-content\"}).span is not None):\n",
    "            profile = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-profile-content\"}).span.contents[0])\n",
    "\n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-benefits-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-benefits-content\"}).span is not None):\n",
    "            benefits = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-benefits-content\"}).span.contents[0])\n",
    "\n",
    "#Die Titel und Textabschnitte werden mit vorangstellter Doppelraute in die neu geöffnete Datei 'Rohtext' geschrieben, die Titel mit Dreifachslashes umgeben zur Separierung\n",
    "    texts = [introduction, description, profile, benefits]\n",
    "    text = open(\"rohtext.txt\", \"a\", encoding='utf-16-le')\n",
    "    text.write(\"\\n\" + \"## \")\n",
    "    text.write(\"///\" + title+ \"///\" + \"\\n\")\n",
    "    for x in texts:   \n",
    "        text.write(x + \"\\n\")\n",
    "    text.close()\n",
    "\n",
    "text = open(\"rohtext.txt\", \"w\", encoding='utf-16-le')\n",
    "text.close()\n",
    "i = 0\n",
    "for url in matches:\n",
    "    offer_url = \"https://www.stepstone.de\" + url #die extrahierten Linkbestandteile der Anzeigen werden durch den Stammlink komplettiert\n",
    "\n",
    "    #Definition des Fake-Headers zur Verschleierung der Anfragen\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36', \"Upgrade-Insecure-Requests\": \"1\",\"DNT\": \"1\",\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\"Accept-Language\": \"de-DE,de;q=0.5\",\"Accept-Encoding\": \"gzip, deflate\"}\n",
    "\n",
    "    sourcecode_offer = requests.get(offer_url, headers=headers).text\n",
    "    soup_offer = BeautifulSoup(sourcecode_offer, \"lxml\")\n",
    "\n",
    "    extract(soup_offer)\n",
    "    i += 1\n",
    "    \n",
    "    #als Zusatz für die Übersichtlichkeit (und weil es cool ist :) ) wird der Fortschritt der Quelltextextraktion aus den Anzeigen dargestellt\n",
    "    print(str(i) + \"/\" + str(len(matches)) + \"---\" + format((i/len(matches))* 100, '.2f') + \"%\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterieren über Links zur Extraktion der Quellcodeabschnitte\n",
    "\n",
    "Da der Prozess der Extraktion der gewünschten Quelltextbestandteile mehrere, sich für jede Anzeige wiederholende Schritte umfasst, bietet es sich an, wieder eine Funktion dafür zu definieren. Diese greift nun den geparsten Quellcode auf und ermöglicht es, über die html-tags auf spezifische Bestandteile des Textes zuzugreifen und diese zu extrahieren. Wie eingangs erklärt, ist hierfür eine genaue Begutachtung der html-Struktur erforderlich. Dabei ergibt sich, dass alle Anzeigen dieselbe Struktur aufweisen und dieselben tags auf die entsprechenden Inhalte verweisen. Bei StepStone handelt es sich um die zu Beginn der Definition angeführten Kategorien, die zunächst initialisiert werden müssen. Dann wird zuerst geprüft, ob ein Titel vorhanden ist. Falls nicht, wird übersprungen, ansonsten wird der Titel extrahiert. Sämtliche für uns interessante Inhalte werden von dem Attribut \"data-at\" umspannt. Der Inhalt wird jeweils entnommen und in einen string gecastet. Dies geschieht für alle Kategorien, also neben dem Titel die Einleitung, Beschreibung, das Bewerber:innenprofil und die Vorteile in jeder Anzeige. Dann wird eine Textdatei geöffnet, in welcher die entnommenen strings jeweils mit einem Zeilenumbruch dazwischen hineingeschrieben werden. Die Titel werden jeweils von drei Slashes umrahmt, um sie vom Textkörper abzugrenzen. Jede neue Anzeige beginnt mit zwei Rauten. Bei jedem neuen Ausführen des Skripts wird die alte Textdatei überschrieben, sodass es nicht zu Dopplungen kommt. Der Fortschritt dieses Prozesses wird zum Schluss noch in ganzzahligen Anteilen sowie in Prozent angegeben. Das Ergebnis ist dann also eine Textdatei mit den extrahierten Inhalten aller Anzeigen, mit noch enthaltenen html-Tags, welches in der definierten Datei im Ordner, in welchem sich auch das Skript befindet, abgelegt wurde. Möglich wäre auch, den gesamten String einfach in ein Objekt zu überführen und damit weiterzuarbeiten. Eine Textdatei hat jedoch den Vorteil, dass man sie auch mit anderen Programmen weiterverarbeiten kann.\n",
    "\n",
    "\n",
    "### Alternative: Manuelles Einlesen der Links\n",
    "\n",
    "Der folgende Code zeigt nun die Variation des händischen Einlesens eines Dokuments mit zusammenkopierten Links, sollten die automatisierten Anfragen von der Website geblockt werden. Die zugrundeliegende Funktion ist die gleiche, es müssen in der zweiten Hälfte des Blocks jedoch einige Anpassungen vorgenommen werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erneutes Importieren der benötigten Module, weil erster Codeblock nicht ausgeführt wird\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "def extract(soup_offer_text):\n",
    "    title = \"\"\n",
    "    introduction = \"\"\n",
    "    description = \"\"\n",
    "    profile = \"\"\n",
    "    benefits = \"\"\n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"header-job-title\"}) is None):\n",
    "        return \n",
    "    else:\n",
    "        title = str(soup_offer_text.find(attrs={\"data-at\": \"header-job-title\"}).contents[0]) \n",
    "    \n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-introduction-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-introduction-content\"}).span is not None):\n",
    "            introduction = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-introduction-content\"}).span.contents[0]) \n",
    "\n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-description-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-description-content\"}).span is not None):\n",
    "            description = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-description-content\"}).span.contents[0])\n",
    "\n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-profile-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-profile-content\"}).span is not None):\n",
    "            profile = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-profile-content\"}).span.contents[0])\n",
    "\n",
    "\n",
    "    if (soup_offer_text.find(attrs={\"data-at\": \"section-text-benefits-content\"}) is not None):\n",
    "        if (soup_offer_text.find(attrs={\"data-at\": \"section-text-benefits-content\"}).span is not None):\n",
    "            benefits = str(soup_offer_text.find(attrs={\"data-at\": \"section-text-benefits-content\"}).span.contents[0])\n",
    "\n",
    "    texts = [introduction, description, profile, benefits]\n",
    "    text = open(\"rohtext.txt\", \"a\", encoding='utf-16-le')\n",
    "    text.write(\"\\n\" + \"## \")\n",
    "    text.write(\"///\" + title+ \"///\" + \"\\n\")\n",
    "    for x in texts:   \n",
    "        text.write(x + \"\\n\")\n",
    "    text.close()\n",
    "\n",
    "text = open(\"rohtext.txt\", \"w\", encoding='utf-16-le')\n",
    "text.close()\n",
    "i = 0\n",
    "\n",
    "#Das Textdokument mit den händisch kopierten Links wird eingelesen\n",
    "with open(\"Links_manuell.txt\") as read_file:\n",
    "     file = read_file.read()\n",
    "#der Gesamttext wird an den Zeilenumbrüchen getrennt, um auf die einzelnen Links zugreifen zu können    \n",
    "for url in file.split('\\n'): \n",
    "    offer_url = url \n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36', \"Upgrade-Insecure-Requests\": \"1\",\"DNT\": \"1\",\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\"Accept-Language\": \"de-DE,de;q=0.5\",\"Accept-Encoding\": \"gzip, deflate\"}\n",
    "    \n",
    "    #die Anzahl muss händisch definiert werden, da sie für die Berechnung nicht mehr aus dem ersten Block abgerufen werden kann\n",
    "    matches = 500 \n",
    "\n",
    "    sourcecode_offer = requests.get(url, headers=headers).text\n",
    "    soup_offer = BeautifulSoup(sourcecode_offer, \"lxml\")\n",
    "\n",
    "    extract(soup_offer)\n",
    "    i += 1\n",
    "    #die len-Funktion wird entfernt, da die Zahl zur Berechnung direkt übernommen wird und die Matches nicht mehr als Liste vorliegen\n",
    "    print(str(i) + \"/\" + str(matches) + \"---\" + format((i/matches)* 100, '.2f') + \"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zweite Hälfte – vom Rohtext zur Analyse\n",
    "\n",
    "Um am Ende ein gut analysierbares Textkorpus zu erhalten, muss die Textmenge noch bereinigt und von den html-Tags sowie überschüssigen Zeichen und Zeilen befreit werden. Dies geschieht zum großen Teil über RegEx, sodass das erforderliche Modul sicherheitshalber am Anfang noch einmal importiert wird, falls der erste Teil des Skripts nicht vorher ausgeführt wurde. Dann wird die Datei mit dem Rohtext eingelesen. Die folgenden Schritte zur Bereinigung sind auf den StepStone-Quelltext zugeschnitten, diesen Teil des Skripts könnte man jedoch mit kleinen Anpassungen auch auf beliebige andere Textmengen in html anwenden, die bereinigt werden sollen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "#einlesen der im vorherigen Schritt erstellten Datei\n",
    "text = open(\"rohtext.txt\", \"r\", encoding='utf-16-le') \n",
    "lines = text.readlines()\n",
    "\n",
    "korpus = \"\".join(lines)\n",
    "    \n",
    "#sämtliche html-Tags entfernen und durch Leerzeichen ersetzen\n",
    "remove = [\"<p>\", \"</p>\", \"<strong>\", \"</strong>\", \"<ul>\", \"</ul>\", \"<li>\", \"</li>\", \"<em>\", \"</em>\", \"<br/>\", \"</a>\",\"<b>\", \"</b>\", \"<u>\", \"</u>\", \"<i>\", \"</i>\"]\n",
    "for x in remove:\n",
    "  korpus = korpus.replace(x, \" \")\n",
    "\n",
    "#Hyperlink-Tags entfernen\n",
    "korpus = re.sub(r'<a href=\".*\">', \"\", korpus)\n",
    "\n",
    "#Zwischenüberschriften-Tags entfernen\n",
    "korpus = re.sub(r'<h1>.*</h1>', \"\", korpus)\n",
    "\n",
    "#Umbrüche durch Leerzeichen ersetzen\n",
    "korpus = korpus.replace(\"\\n\", \" \")\n",
    "\n",
    "#entfernt zu viele und merkwürdig codierte Leerzeichen\n",
    "korpus = korpus.replace(\"   \", \" \")\n",
    "while \"  \" in korpus:\n",
    "    korpus = korpus.replace(\"  \", \" \")\n",
    "\n",
    "#ersetzt alles, was bei manueller Durchsicht an komischen Zeichen aufgefallen ist, durch die gängigen Equivalente\n",
    "korpus = korpus.replace(\"„\", \"\\\"\")\n",
    "korpus = korpus.replace(\"“\", \"\\\"\")\n",
    "korpus = korpus.replace(\"·\", \"-\")\n",
    "korpus = korpus.replace(\"‑\",\"-\")\n",
    "korpus = korpus.replace(\"’\",\"'\")\n",
    "korpus = korpus.replace(\"…\",\"...\")\n",
    "korpus = korpus.replace(\"\\t\",\" \")\n",
    "\n",
    "#Um auf Nummer sicher zu gehen, entfernt das hier nochmal alles, was nicht in den eckigen Klammern ist\n",
    "korpus = re.sub(r'[^a-zA-Z0-9äöü.,;ß/()\\[\\]\\-#\\|&\\*\\+:ÖÄÜ?\"!§$%&\\\\²³€\\'\\s]', \"\", korpus)\n",
    "\n",
    "#der bereinigte String wird in eine neu geöffnete Datei geschrieben\n",
    "text = open(\"Dachau.txt\", \"w\", encoding='utf-16-le')   \n",
    "text.write(korpus)\n",
    "text.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ansätze für die quantitative Korpusanalyse\n",
    "\n",
    "Nachdem die Anzeigentexte nach den gewünschten Parametern gescraped, zu einem Textkorpus zusammengefügt und bereinigt wurden, kann nun mit der eigentlichen Analyse begonnen werden. Dabei ist es wichtig, beim Preprocessing genau zu überlegen, was untersucht werden soll. Wenn es zum Beispiel um Lemmata geht, ist es sinnvoll, einheitliche Kleinschreibung und Bereinigung von sämtlichen Zeichen durchzuführen, während bei der Untersuchung von Genderformen eben dies kontraproduktiv wäre, da in dem Falle Genderzeichen und Binnenmajuskel zum Opfer fielen. \n",
    "\n",
    "Da es bei der Zusammenstellung dieses Korpus darum gehen sollte, verschiedene Arten des Genderns in Stellenanzeigen zu untersuchen, werden in den folgenden Codeblöcken nun einige Möglichkeiten der Weiterverarbeitung und Analyse gezeigt, um sich dem Untersuchungsgegenstand zu nähern. Ein zentraler Bestandteil stellt die Sortierung nach verschiedenen Genderarten dar. Für diesen Zweck eignet sich am Besten die Aufteilung in unterschiedliche dictionaries, eines für jede Art des Genderns. Zunächst werden jedoch die gesamten Stellenanzeigen in eine dictionary-Struktur gebracht, mit den Titeln als keys und den Jobbeschreibungen als values.\n",
    "\n",
    "Im nächsten Schritt erfolgt nun die Aufteilung nach Arten des Genderns. Entsprechende RegEx filtern die Titel, welche die gewünschten Muster enthalten, heraus. Danach werden sie in dictionaries abgelegt und aus dem Gesamtdictionary (job_text) entfernt. Wichtig ist hierbei die Reihenfolge im Code: da viele Anzeigentitel mehrere Genderarten enthalten (zum Beispiel \"Mitarbeiter* m/w/d\") muss entschieden werden, in welches dictionary dieser Titel sortiert wird, oder eher gesagt: Welche Art des Genderns \"relevanter\" ist. Da ein einzelnes Sternchen deutlich seltener vorkommt als der Zusatz m/w/d, sollte das Beispiel also im Sternchen-dictionary landen. Aus diesem Grund muss die Sternchen-Sortierung vor der m/w/d-Sortierung stattfinden, da der Eintrag nur einem dictionary zugeordnet wird und zwar dem, dessen RegEx es als erstes matcht. Dieses Vorgehen ist natürlich ganz von den jeweiligen Fragestellungen und dem Textmaterial abhängig und kann entsprechend variiert werden.\n",
    "\n",
    "Nachdem sämtliche Titel in die jeweils passenden dictionaries sortiert wurden, wird das Ergebnis dann entsprechend ausgegeben. Die Titel, sortiert nach ihrer Genderart, sowie ihre Anzahl und die Gesamtzahl der Jobs. Zudem wird auch der \"Rest\" ausgegeben, also alle Einträge, die nach der Sortierung noch im ursprünglichen dictionary verblieben sind. So können auch abweichende Varianten oder verrückte Sonderformen, die nicht von den RegEx gematcht werden, angezeigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n",
      "sternchen\n",
      "Software Developer* Cloud-Technologien\n",
      "Cloud Architect* (AWS, Azure, GCP, private Cloud)\n",
      "-----\n",
      "in-Formen\n",
      "Vertriebler:in / Berater:in von Immobilieneigentümer:innen / Sales Multimedia (Erfahrung im Außendienst)\n",
      "Category Insights Manager:in DE (m/w/d)\n",
      "Ingenieur*in als Sachverständige*r Anlagensicherheit zur Ausbildung (ZÜS Druck- und Ex-Anlagen, Notifizierte Stelle, Funktionale Sicherheit)\n",
      "Projektingenieur*in für die Beleuchtung von Jugendspieleinrichtungen (w/m/d)\n",
      "Versuchsingenieur*in für Sicherheitsprüfungen\n",
      "Ingenieur*in / Techniker*in im Bereich Versorgungstechnik\n",
      "Ingenieur*in als Gutachter*in im Bereich Anlagensicherung\n",
      "Sachbearbeiter*in HR Vertragsmanagement\n",
      "Junior Versuchstechniker*in für Sicherheitsprüfungen\n",
      "Projektingenieur*in im Klärwerksbau (w/m/d)\n",
      "Gleisbauer:in / Fachlagerist:in\n",
      "-----\n",
      "m/d/w-Formen\n",
      "Elektrokonstrukteur (m/w/d)\n",
      "SAP Analytics Cloud (SAC) Junior Consultant (m/w/d)\n",
      "Mitarbeiter/in (m/w/d) für die Kundenbetreuung im Außendienst Region Tübingen\n",
      "Mitarbeiterin/Mitarbeiter der Vermessungstechnik oder der Geomatik (m/w/d)\n",
      "Informatiker, Ingenieure und Naturwissenschaftler für dieSoftwareentwicklung (m/w/d)\n",
      "Microsoft SharePoint Developer (m/w/d)\n",
      "Ingenieure, Techniker, Technische Produktdesigner, Konstrukteure(m/w/d)\n",
      "Microsoft CRM Developer (m/w/d)\n",
      "Servicetechniker (m/w/d) Elektro im Facility Management\n",
      "Konstrukteur (m/w/d)\n",
      "Auditor für globale Prozesse und Projekte (m/w/d)\n",
      "SAP Consultant Solution Manager - Support (m/w/d)\n",
      "Mitarbeiter/in (m/w/d) für die Kundenbetreuung im Außendienst Region Würzburg\n",
      "Elektriker im Wechselschichtdienst (m/w/d)\n",
      "Monteur HLSK (m/w/d)\n",
      "Technischer Ausbilder (m/w/d)\n",
      "Softwareentwickler (m/w/d)\n",
      "Mitarbeiter/in (m/w/d) für die Kundenbetreuung im Außendienst Region Mannheim\n",
      "Projektleiter (m/w/d) Order Management\n",
      "Project Manager (m/w/d) Additive Manufacturing\n",
      "Gruppenleiter Parkleitstelle (m/w/d)\n",
      "Application Engineers (m/w/d) für die Bereiche Coating und Imprint Processes\n",
      "IT 1st & 2nd Level Support (m/w/d)\n",
      "Technischer Support Batteriespeichersysteme (m/w/d) im Großraum Dortmund oder München\n",
      "Software Architekt .NET / C# Software Engineer (m/w/d)\n",
      "IT24 Expert Container Technology - Software development environment (d/m/w)\n",
      "HR IT Developer (w/m/d) HCM Cloud Master Data & Payroll Mapping\n",
      "(Senior) IT Security Engineer (m/w/d)\n",
      "Service Package Owner Software Management (m/w/d)\n",
      "HR IT Developer (w/m/d) HCM Personnel Administration & Organizational Management\n",
      "Softwareentwickler Java (m/w/d)\n",
      "Senior Frontend Developer Angular / React (w|m|d)\n",
      "IT Consultant - Weiterentwicklung zum Cyber Defender w/m/d\n",
      "C++ Software-Entwickler (m/w/d) Microwave Imaging Products\n",
      "Product Manager Bonding Systems (m/w/d)\n",
      "Softwareentwickler (m/w/d) Fahrzeugsimulation\n",
      "Anwendungsingenieur X-Ray (m/w/x)\n",
      "Anwendungsingenieur Multisensortechnik (m/w/x)\n",
      "Data Warehouse Engineer (w/m/d)\n",
      "Fachinformatiker für IT: Umschulung -/ Weiterbildung-/ Quereinsteiger (m/w/d)\n",
      "Medizintechniker, IT-System-Elektroniker, Mechatroniker, Elektroniker in Vollzeit (m/w/d)\n",
      "Executive Assistant, Mechatronics & Sustainable Packaging (m/f/d)\n",
      "Senior Key Account Manager, Delivery Service Partners (DSP) (m/f/d)\n",
      "Senior Manager - Business Intelligence Engineers (m/f/d)\n",
      "Program Manager Cost to Serve and IR Operations, EU SAS Business Operations (m/f/d)\n",
      "Leistungs- und Antrags-/Risikoprüfer Lebensversicherung/Biometrie (m/w/d)\n",
      "Office Manager / Workplace Experience Manager (m/w/d)\n",
      "Chefarzt Kardiologie (m/w/d)\n",
      "Projektleiter für Veranstaltungstechnik (m/w/d)\n",
      "Fachkraft für Veranstaltungstechnik (m/w/d)\n",
      "Sales & Marketing Assistant (m/w/d)\n",
      "Elektroniker Gebäudetechnik m/w/d\n",
      "SPS-Programmierer (m/w/d) | Softwareprogrammierer SPS (m/w/d) | Automatisierungsingenieur (m/w/d)\n",
      "Service Desk Mitarbeiter Schwerpunkt Versicherten-Helpdesk (m/w/d)\n",
      "Produktionsmitarbeiter Anlagenbedienung (w/m/d)\n",
      "Chemielaborant / CTA / B. Sc. Präparative Chemie (w/m/d)\n",
      "Versicherungskaufmann - Kundenberatung Komposit/Teilzeit (m/w/d) Teilzeit (25 Std./Woche, Montag-Freitag vormittags/nachmittags nach Absprache)\n",
      "Projektleiter / Planungsleiter (m/w/d) für Kläranlagen\n",
      "Audio Specialist Sound Tuning (m/w/d)\n",
      "Data Engineer mit Schwerpunkt Customer Centricity (w/m/d)\n",
      "MSR-Techniker Gebäudetechnik (m/w/d) Willkommensbonus\n",
      "Contract Manager (w/m/d)\n",
      "Consultant Accounting & Controlling mit Business Analytics - Fast Track Trainee Programm (w/m/d)\n",
      "KFZ-Hochvoltmechatroniker (m/w/d) für PKW/VAN in der Mercedes-Benz Niederlassung München, Landsberger Straße 382, München\n",
      "Business Continuity Manager (m/w/d)\n",
      "IT Service Manager Software Management (m/w/d)\n",
      "Projekteinkäufer (m/w/d) Automotive\n",
      "Red Hat OpenShift Architekt (m/w/d)\n",
      "Teamassistenz - Service Desk Agent m/w/d\n",
      "Supply Chain / Procurement Excellence Specialist (m/w/d)\n",
      "Assistant Sales Manager / Vertriebsinnendienst (m/w/d)\n",
      "Senior Consultant Projektmanagement Anlagenbau/Automotive (m/w/d)\n",
      "Master Data Specialist (m/w/d)\n",
      "Firewalladministrator bei BurdaSolutions (m/w/d)\n",
      "Wholesale Support Analyst w/m/d\n",
      "Administrative Assistant (m/w/d) Customer Office Garching\n",
      "Produktexperte für den PKW Neufahrzeugverkauf (m/ w/d) - Mercedes-Benz AG Niederlassung München Standort Arnulfstraße\n",
      "(Senior) Consultant Manufacturing - Life Science (w|m|d)\n",
      "Consultant Dynamics CRM (m/w/d)\n",
      "Spezialsachbearbeiter/ Mitarbeiter (m/w/d) Organisationsmanagement SAP HCM\n",
      "Technical Product Owner Connected AI Platform (w/m/x)\n",
      "Trainee Corporate Banking (m/w/d) - Analyst Corporate Credit - UNICREDIT GRADUATE PROGRAM\n",
      "Facharbeiter Elektrik/Mechanik im Schichtdienst (m/f/d)\n",
      "(Junior-) Projektcontroller (m/w/d) für Architekturprojekte\n",
      "Anlagenmechaniker für Sanitär- Heizungs- und Klimatechnik (m/w/d) Willkommensbonus\n",
      "Personalsachbearbeiter (m/w/d)\n",
      "Serieneinkäufer (m/w/d) Automotive\n",
      "Referent Interne Revision / Senior Internal Auditor (m/w/d)\n",
      "Anlagenmechaniker als Servicetechniker m/w/d\n",
      "Agile Coach (w|m|d)\n",
      "Teamleitung (m/w/d) Mediaplanung\n",
      "Systemintegrator - Änderungsmanagement (w/m/x)\n",
      "Bauzeichner / Bautechniker / CAD-Konstrukteur (m/w/d) für Wasserversorgung\n",
      "Facharzt/-ärztin für Arbeitsmedizin (w/m/d)\n",
      "Sachbearbeitung Personalentwicklung (m/w/d) Vollzeit, Teilzeit\n",
      "Junior Sales Project Manager (m/w/d)\n",
      "Service Manager (m/w/x) Major Incident Management\n",
      "Facharzt (m/w/d) für Psychiatrie & Psychotherapie - bis zu 16.500 € Grundgehalt\n",
      "Consultant Digital Transformation Accounting & Controlling - Business Consulting Finance (w/m/d)\n",
      "Teamleitung Gartenbau (m/w/d) im Inklusionsbetrieb\n",
      "Inside Sales Manager / Mitarbeiter Vertriebsinnendienst (m/w/d)\n",
      "Global Process Owner Workday Absence & Time Tracking (f/m/x)\n",
      "Global Process Owner Workday Core (f/m/x)\n",
      "Kundeberater (m/w/d) in der Edelmetallbranche\n",
      "Senior Sales Manager, Amazon Freight (m/f/d)\n",
      "Augenoptiker / Verkaufstalente (m/w/d)\n",
      "Product Manager (w/m/d) Wohnen und Accessoires\n",
      "Elektriker/Mechatroniker Professional als Servicetechniker m|w|d im Außendienst - Region München, Südbayern\n",
      "IT Officer (m/f/d)\n",
      "Immobilienmakler (m/w/d) in Festanstellung mit Fixgehalt + ungedeckelter Provision\n",
      "Projektassistenz (m/w/d) in München\n",
      "Werkstudent Legal & Compliance (m/w/d)\n",
      "Vertriebsleiter Region DACH für technische Investitionsgüter (m/w/d)\n",
      "Pharmareferent (m/w/d) im Arztaußendienst ( M26)\n",
      "Projektleiter Medien TGA (m/w/d) Heizungs- und Klimatechnik oder Sanitär\n",
      "Online und Print Redakteur/in für B2B High-Tech (m/w/d)\n",
      "Manager Independence & Corporate Governance (m/w/d)\n",
      "-----\n",
      "Rest\n",
      "kaufmännischer Angestellter / Steuerfachangestellter für den Bereich Buchhaltung und Büromanagement gesucht\n",
      "Software Development Engineer\n",
      "Software-Ingenieur Lenkflugkörper (gn)\n",
      "IT-Projektmanager (gn)\n",
      "Project Engineer Healthcare\n",
      "SAP SD E-Invoicing Process Consultant für S/4HANA Transformation\n",
      "Umweltschutzmanager (all genders)\n",
      "Programme Manager, Amazon Logistics Launch & Expansion\n",
      "Jobs insg.: 138\n",
      "Jobs Sternchen: 2\n",
      "Jobs *_:in: 11\n",
      "Jobs m/w/d: 117\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "#Die entsprechende (Teil-)Korpusdatei wird eingelesen\n",
    "korpus = open(\"Würzburg.txt\", \"r\", encoding='utf-16-le') \n",
    "lines = korpus.readlines()\n",
    "\n",
    "korpus = \"\".join(lines)\n",
    "korpus = korpus.strip()\n",
    "\n",
    "korpus = korpus.split(\"##\")\n",
    "print(len(korpus))\n",
    "job_text = {} #das dictionary für alle Stellenanzeigentexte wird initiiert\n",
    "korpus.remove('') #Leere Einträge werden entfernt\n",
    "\n",
    "title_regex = r'///.*///'\n",
    "for x in korpus:\n",
    "    inseration = x.strip()\n",
    "    title = str(re.findall(title_regex, inseration).pop())\n",
    "\n",
    "    #Titel und Stellenbeschreibung werden getrennt, mit Titeln als keys und den Beschreibungen als values\n",
    "    inseration = inseration.replace(title,\"\")\n",
    "    inseration.strip()\n",
    "    title = title.replace(\"///\",\"\")\n",
    "    job_text[title] = inseration\n",
    "num_jobs = len(job_text)\n",
    "\n",
    "\n",
    "#Verschiedene dictionaries für die verschiedenen Genderformen\n",
    "print(\"sternchen\") #reines Sternchen ohne folgendes Suffix\n",
    "sternchen_dict = {}\n",
    "sternchen_regex =r'.\\*(\\s|\\))'\n",
    "\n",
    "for title in job_text:\n",
    "    if len(re.findall(sternchen_regex, title)) != 0:\n",
    "        print(title)\n",
    "        sternchen_dict[title] = job_text[title]\n",
    "\n",
    "#die gematchten Titel werden aus dem Gesamtdictionary entfernt und ein Trennstrich ausgegeben\n",
    "for title in sternchen_dict:\n",
    "    job_text.pop(title, None)\n",
    "print(\"-----\") \n",
    "\n",
    "print(\"in-Formen\")\n",
    "in_dict = {} # für alle *in, :in, _in\n",
    "in_regex = r'\\w*(\\*|:|_)\\s*in'\n",
    "\n",
    "for title in job_text:\n",
    "    if len(re.findall(in_regex, title)) != 0:\n",
    "        print(title)\n",
    "        in_dict[title] = job_text[title]\n",
    "\n",
    "for title in in_dict:\n",
    "    job_text.pop(title, None)\n",
    "print(\"-----\")     \n",
    "\n",
    "print(\"m/d/w-Formen\")\n",
    "mwd_dict = {} #für alle (m/w/d), (w/m/d), etc. - Formen\n",
    "mwd_regex = r'(\\(|\\s)(\\w(\\/|\\||,)\\s*\\w(\\/|\\||,)\\s*\\w)|(\\w(\\/|\\||,)\\s*\\w)(\\)|\\s)'\n",
    "\n",
    "for title in job_text:\n",
    "    \n",
    "    if len(re.findall(mwd_regex, title)) != 0:\n",
    "        print(title)\n",
    "        mwd_dict[title] = job_text[title]\n",
    "\n",
    "\n",
    "for title in mwd_dict:\n",
    "    job_text.pop(title, None)\n",
    "print(\"-----\")\n",
    "\n",
    "print(\"Rest\")\n",
    "for title in job_text:\n",
    "    print(title)\n",
    "print(\"Jobs insg.: \" + str(num_jobs))\n",
    "print(\"Jobs Sternchen: \" + str(len(sternchen_dict)))\n",
    "print(\"Jobs *_:in: \" + str(len(in_dict)))\n",
    "print(\"Jobs m/w/d: \" + str(len(mwd_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies ist natürlich nur ein einziges Beispiel für einen Analyseansatz. Je nach Fragestellungen können sämtliche Skriptblöcke beliebig angepasst werden und bieten umfassende Untersuchungsmöglichkeiten, wie Lemmataanalysen, Wortformen und deren relative Anteilen, Kollokationen und Ähnliches. Durch die Strukturierung des Korpus als dictionary sind auch Kookurrenzen und Auffälligkeiten zwischen den Titel und Textbestandteilen möglich, beispielsweise, ob es Zusammenhänge gibt zwischen der Genderform des Titels und der verwendeten Lexik der Texte. \"Sind die Texte unter dem mit dem Mindeststandard m/w/d gegenderten Titel im 'generischen' Maskulinum verfasst?\" \"Bedeuten Sternchenformen im Titel auch Sternchenformen im Text?\" wären mögliche Fragestellungen, denen so nachgegangen werden könnte.   \n",
    "\n",
    "## Fazit\n",
    "Die vorliegende Arbeit sollte einen beispielhaften Weg nachzeichnen, wie ein einfaches Programm geschrieben werden kann, um Texte eines Online-Stellenportals automatisiert zu extrahieren und schließlich ein Textkorpus zu erstellen, welches bereinigt und analysiert wird. Dabei ist zu beachten, dass die erste Hälfte des Programms sehr spezifisch auf die Website StepStone zugeschnitten ist und erst einmal nicht generalisiert für alle Stellenportale angewendet werden kann. Durch entsprechende Analyse des jeweiligen Quellcode-Aufbaus der Seiten sowie der html-Strukturen kann der Code jedoch entsprechenden angepasst werden und somit als Vorlage dienen. Die zweite Hälfte kann flexibel für simple Textdokumente unabhängig von ihrer Quelle verwendet werden und macht den Code somit auch ohne den Bestandteil des Scrapings vielseitig einsetzbar.\n",
    "\n",
    "Wie gezeigt wurde, sind die Abwehrmechanismen solcher Websites nicht zu unterschätzen. StepStone ist ein Beispiel dafür, wie viele, teils schwer zu durchschauende Fallen im Code der Seite eingebaut sein können, welche einen automatisierten Zugriff behindern oder unmöglich machen. Dies ist bei der Planung des Projekts zu berücksichtigen. Zudem handelt es sich beim Webscraping um eine rechtliche Grauzone, bei der es zu Schwierigkeiten kommen kann, wenn die gesammelten Daten zum Beispiel veröffentlicht oder für kommerzielle Zwecke genutzt werden. Auch hier empfiehlt es sich, vorher Nachforschungen bezüglich der Bedingungen, Möglichkeiten und Grenzen der jeweiligen Website anzustellen.\n",
    "\n",
    "Sind diese Hürden überwunden, bietet ein solches Programm aber einen enorm zeitsparenden und effizienten Weg, große Textmengen nach sehr individuellen Kriterien zu sammeln sowie schnell und zu sauber bereinigen und – last but not least – auch nach den verschiedensten Gesichtspunkten quantitativ analysieren zu können. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliographie\n",
    "\n",
    "### Primärquellen\n",
    "\n",
    "https://www.stepstone.de/, Stand 01.04.2024.\n",
    "\n",
    "### Sekundärquellen \n",
    "\n",
    "Emre (2023): Wie man Webscraping verhindert: Schützen Sie Ihre Website. https://de.webscraping.blog/wie-man-web-scraping-verhindert/, Stand 18.03.2024.\n",
    "\n",
    "Fett, Daniel (2006): Tutorial Reguläre Ausdrücke. https://danielfett.de/2006/03/20/regulaere-ausdruecke-tutorial/, Stand 20.03.2024.\n",
    "\n",
    "Ikleiw, Aleksander (2020): *Antwort auf einen Beitrag* (ohne Titel) bei www.stackoverflow.com. https://stackoverflow.com/questions/62973647/get-request-works-with-postman-but-not-with-python-requests-and-curl, Stand 20.03.2024.\n",
    "\n",
    "McKinney, Wes (2023): Datenanalyse mit Python. Auswertung von Daten mit pandas, NumPy und Jupyter. 3. Auflage. Heidelberg: O'Reilly.\n",
    "\n",
    "pandas (2024): Working with text data. https://pandas.pydata.org/docs/user_guide/text.html, Stand 19.03.2024.\n",
    "\n",
    "Python Software Foundation (2001–2024): Python HOWTOs. https://docs.python.org/3/howto/index.html, Stand 01.04.2024.\n",
    "\n",
    "Richardson, Leonard (2004–2023): Beautiful Soup Documentation. https://www.crummy.com/software/BeautifulSoup/bs4/doc/, Stand 10.03.2024.\n",
    "\n",
    "\n",
    "\n",
    "### Tools \n",
    "\n",
    "``ChatGPT``\n",
    "\n",
    "Prompts: \n",
    "\n",
    "+ How to prevent blocking mechanisms for web scraping?\n",
    "\n",
    "+ Why does the http-request takes so long without giving an error?\n",
    "\n",
    "+ Why does the following RegEx ^href=\".*stellenangebote-\\S+\"$ match the searched link in a trial when the link is seperately tested but not when I try to apply it to the whole source code?\n",
    "\n",
    "+ \"extract(soup_offer)\n",
    "    i += 1\n",
    "    print(str(i) + \"/\" + str(len(matches)) + \"---\" + format((i/len(matches))* 100, '.2f') + \"%\")?\"\n",
    "    Why does it add up to 125/25 and therefore calculates a progress of 500%?\n",
    "\n",
    "``RegExr``\n",
    "\n",
    "+ diverses Durchprobieren sämtlicher verwendeter RegEx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
